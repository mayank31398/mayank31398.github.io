<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Mayank Mishra</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Mayank Mishra" />

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700">
    <link rel="stylesheet" href="css/animate.css">
    <link rel="stylesheet" href="css/icomoon.css">
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/flexslider.css">
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/style.css">
    <script src="js/modernizr-2.6.2.min.js"></script>
</head>

<body>
    <div class="sticky">
        <canvas id=c></canvas>
    </div>

    <div id="colorlib-page">
        <div>
            <a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar"
                aria-expanded="false" aria-controls="navbar"><i></i></a>
            <aside id="colorlib-aside" role="complementary" class="border js-fullheight">
                <div class="text-center">
                    <div class="author-img" style="background-image: url(images/personal/me.jpg);"></div>
                    <h1 id="colorlib-logo"><a href="index.html">Mayank Mishra</a></h1>
                    <span class="position">
                        <a href="http://www.iitd.ac.in/">Indian Institute of Technology Delhi</a>
                    </span>
                </div>
                <nav id="colorlib-main-menu" role="navigation" class="navbar">
                    <div id="navbar" class="collapse">
                        <ul>
                            <li><a href="#" data-nav-section="about" class="navbar-item">About</a></li>
                            <li><a href="#" data-nav-section="publications" class="navbar-item">Publications</a></li>
                            <li><a href="#" data-nav-section="patents" class="navbar-item">Patents</a></li>
                        </ul>
                    </div>
                </nav>

                <div class="colorlib-footer">
                    <ul>
                        <li><a href="https://github.com/mayank31398"><i class="icon-github"></i></a></li>
                        <li><a href="https://www.linkedin.com/in/mayank31398"><i class="icon-linkedin22"></i></a></li>
                        <li><a href="https://scholar.google.com/citations?user=YsbtW6cAAAAJ&hl=en"><img
                                    src="images/icons/google_scholar_icon.png" style="width:18px;height:18px;"></a></li>
                        <li><a href="https://x.com/MishraMish98"><img src="images/icons/x.png"
                                    style="width:18px;height:18px;"></a></li>
                        <li><a href="https://huggingface.co/mayank-mishra"><img src="images/icons/huggingface.png"
                                    style="width:18px;height:18px;"></a></li>
                        <!-- <li><a href="https://www.instagram.com/asuna_fps.mayank/"><i class="icon-instagram"></i></a>
                        </li>
                        <li><a href="https://www.youtube.com/channel/UCbu9MmNckvyhnCFILfrGlig"><i
                                    class="icon-youtube"></i></a></li>
                        <li><a href="https://www.twitch.tv/asuna_fps"><i class="icon-twitch"></i></a></li> -->
                    </ul>
                </div>
            </aside>
        </div>

        <div class="container-wrap">
            <div id="colorlib-main">
                <section class="colorlib-about" data-section="about">
                    <div class="colorlib-narrow-content">
                        <div class="row">
                            <div class="col-md-12">
                                <div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInRight">
                                    <div class="col-md-12">
                                        <div class="about-desc">
                                            <h2 class="colorlib-heading">About Me</h2>
                                            <p>
                                                Hi I'm <strong>Mayank Mishra</strong>.
                                                <br><br>
                                                I am currently working as a Research Engineer at IBM Research India. I
                                                am a B.Tech graduate in Electrical Engineering from <strong>Indian
                                                    Institute of Technology Delhi</strong>.
                                                <br>
                                                My research interests include <strong>Quantum
                                                    Computing</strong>, <strong>Deep Learning</strong>,
                                                <strong>Reinforcement Learning</strong>, <strong>Natural Language
                                                    Processing</strong>, <strong>Classical AI</strong> and
                                                <strong>Information Theory</strong>.
                                                <br>

                                                I also love listening to music and gaming.
                                            <p><a href="doc/Mayank_Mishra_CV.pdf" target="_blank"
                                                    class="btn btn-primary btn-learn">Resume <i
                                                        class="icon-download4"></i></a></p>
                                            </p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="colorlib-experience" data-section="publications">
                    <div class="colorlib-narrow-content">
                        <div class="row">
                            <div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box"
                                data-animate-effect="fadeInRight">
                                <h2 class="colorlib-heading">Publications</h2>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-md-12">
                                <div class="timeline-centered">
                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner"
                                            id="reducing_transformer_key_value_cache_size_with_cross_layer_attention">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://arxiv.org/abs/2405.12981" style="color: #fffdfe">
                                                        Reducing Transformer Key-Value Cache Size with Cross-Layer
                                                        Attention
                                                    </a>
                                                </h2>
                                                <div style="color: #fffdfe">
                                                    William Brandon, <b>Mayank Mishra</b>, Aniruddha Nrusimha, Rameswar
                                                    Panda, Jonathan Ragan Kelly
                                                    <br>
                                                    arxiv preprint: <a href="https://arxiv.org/abs/2405.12981"
                                                        style="color: #fffdfe">https://arxiv.org/abs/2405.12981</a>
                                                </div>
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner"
                                            id="variational_learning_for_unsupervised_knowledge_grounded_dialogs">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://www.ijcai.org/proceedings/2022/0597"
                                                        style="color: #fffdfe">
                                                        Variational Learning for Unsupervised Knowledge Grounded Dialogs
                                                    </a>
                                                    <span style="float: right">[Published in IJCAI]</span>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Proposed a model to generate responses for dialogs grounded on
                                                        information present in external knowledge sources</li>
                                                    <li>Retrieved the relevant textual documents from a large indexed
                                                        collection of documents in an unsupervised fashion</li>
                                                    <li>Used a variational framework to take advantage of the posterior
                                                        distribution to retrieve better documents during training</li>
                                                    <li>Also showed the efficacy of the proposed model on other tasks
                                                        like question answering, classification etc</li>
                                                    <li>Published in <u><a
                                                                href="https://www.ijcai.org/proceedings/2022/0597">International
                                                                Joint Conferences on Artificial Intelligence (IJCAI
                                                                2022)</a></u></li>
                                                </ul>
                                                <img src="images/papers/variational_learning_for_unsupervised_knowledge_grounded_dialogs.png"
                                                    style="width: 50%; height: 50%">
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://ieeexplore.ieee.org/document/8843916"
                                                        style="color: #fffdfe">
                                                        Adversarial Approximate Inference for Speech to
                                                        Electroglottograph Conversion
                                                    </a>
                                                    <span style="float: right">[Published in IEEE TASLP]</span>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Optimized the Speech to Laryngograph encoder using adversarial
                                                        training for the network using informative priors</li>
                                                    <li>Created a cosine based loss function for enforcing amplitude
                                                        invariance between ground truth and network output</li>
                                                    <li>Used a variational inference approach for learning optimal
                                                        representations for speech signal to infer the EGG signal</li>
                                                    <li>Demonstrated the advantages of using informative priors over
                                                        Gaussian priors in the variational autoencoder setting</li>
                                                    <li>Utilized continuous wavelet transforms using Ricker wavelets for
                                                        robust peak picking</li>
                                                    <li>Published in <u><a
                                                                href="https://ieeexplore.ieee.org/document/8843916">IEEE/ACM
                                                                Transactions on Audio, Speech, and Language Processing
                                                                (TASLP) (Volume 27, Issue 12)</a></u></li>
                                                </ul>
                                                <p>
                                                    <a href="https://arxiv.org/abs/1903.12248">Arxiv preprint</a>
                                                </p>
                                                <img src="images/papers/adversarial_approximate_inference_for_speech_to_electroglottograph_conversion.png"
                                                    style="width: 50%; height: 50%">
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner"
                                            id="bloom_a_176B_parameter_open_access_multilingual_language_model">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://arxiv.org/abs/2211.05100" style="color: #fffdfe">
                                                        BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
                                                    </a>
                                                    <span style="float: right">[Submitted to JMLR]</span>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Contributed to the training <u><a
                                                                href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">codebase</a></u>
                                                        of BLOOM, a 176-Billion parameter multilingual large language
                                                        model</li>
                                                    <li>Implemented state-of-the-art AliBi positional encodings to
                                                        attend over long sequences unseen during training</li>
                                                    <li>Also implemented a novel checkpoint reshaping strategy to change
                                                        the distributed configuration of a large model</li>
                                                    <li>Worked in open collaboration with researchers all over the world
                                                        and gained experienced on large scale trainin</li>
                                                    <li>Submitted to <u>Journal of Machine Learning Research (JMLR)</u>
                                                    </li>
                                                </ul>
                                                <p>
                                                    <a href="https://arxiv.org/abs/2211.05100">Arxiv preprint</a>
                                                </p>
                                                <img src="images/papers/bloom_a_176b_parameter_open_access_multilingual_language_model.png"
                                                    style="width: 50%; height: 50%">
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner"
                                            id="Joint_Reasoning_on_Hybrid-knowledge_sources_for_Task-Oriented_Dialog">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://arxiv.org/abs/2210.07295" style="color: #fffdfe">
                                                        Joint Reasoning on Hybrid-knowledge sources for Task-Oriented
                                                        Dialog
                                                    </a>
                                                    <span style="float: right">[Submitted to JMLR]</span>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Worked on generating agent responses to conversations requiring
                                                        reasoning over both structured databases and unstructured text
                                                        documents</li>
                                                    <li>Created a modified version of the MultiWOZ dataset and showed
                                                        that existing methods failed on the created dataset</li>
                                                    <li>Proposed a baseline model trained with Prompt+LM tuning to
                                                        retrieve the relevant information (from both structured and
                                                        unstructured sources) and generate the response</li>
                                                    <li>Submitted to <u>European Chapter for the Association for
                                                            Computational Linguistics (EACL 2023)</u></li>
                                                </ul>
                                                <p>
                                                    <a href="https://arxiv.org/abs/2210.07295">Arxiv preprint</a>
                                                </p>
                                                <img src="images/papers/joint_reasoning_on_hybrid_knowledge_sources_for_task_oriented_dialog.jpg"
                                                    style="width: 50%; height: 50%">
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://arxiv.org/abs/1903.09940" style="color: #fffdfe">
                                                        Variational Inference with Latent Space Quantization for
                                                        Adversarial Resilience
                                                    </a>
                                                    <span style="float: right">[Submitted to AAAI]</span>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Implemented a defense mechanism capitalizing on the expressive
                                                        power of regularized latent space generative models</li>
                                                    <li>Trained Variational Autoencoders with a K-Lipschitz encoder to
                                                        ensure closeness of similar images in the latent space</li>
                                                    <li>Proposed a mechanism for defending neural networks against
                                                        adversarial examples using latent space quantization</li>
                                                    <li>Demonstrated the efficacy of the proposed mechanism against
                                                        multiple attack types (black and white box) and methods</li>
                                                    <li>Submitted to <u>Association for the <u>Advancement of Artificial
                                                                Intelligence (AAAI)</u></li>
                                                </ul>
                                                <p>
                                                    <a href="https://arxiv.org/abs/1903.09940">Arxiv preprint</a>
                                                </p>
                                                <img src="images/papers/variational_inference_with_latent_space_quantization_for_adversarial_resilience.png"
                                                    style="width: 50%; height: 50%">
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry begin animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner">
                                            <div class="timeline-icon color-none">
                                            </div>
                                        </div>
                                    </article>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="colorlib-experience" data-section="patents">
                    <div class="colorlib-narrow-content">
                        <div class="row">
                            <div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box"
                                data-animate-effect="fadeInRight">
                                <h2 class="colorlib-heading">Patents</h2>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-md-12">
                                <div class="timeline-centered">
                                    <article class="timeline-entry animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner">
                                            <div class="timeline-icon color-9">
                                                <i class="icon-paper"></i>
                                            </div>
                                            <div class="timeline-label">
                                                <h2>
                                                    <a href="https://www.ijcai.org/proceedings/2022/0597"
                                                        style="color: #fffdfe">
                                                        EdgeEGG - A system and method for hand-held electrode free
                                                        elctroglottograph using neural networks on programmable
                                                        controllers
                                                    </a>
                                                </h2>
                                                <ul style="color: #fffdfe">
                                                    <li>Proposed a safe contact-free ElectroGlottoGraph which provides
                                                        an accurate estimate of EGG signal</li>
                                                    <li>Proposed a cost-effective and efficient mechanism with
                                                        integrated speech sensors to allow edge computation of EGG</li>
                                                    <li>Designed a resource efficient hardware device optimizing both
                                                        energy consumption and prediction latency, by performing
                                                        computations on very low power micro-controllers</li>
                                                    <li>Also showed the efficacy of the proposed model on other tasks
                                                        like question answering, classification etc</li>
                                                    <li>Indian Patent application, No. 201911036593</li>
                                                </ul>
                                            </div>
                                        </div>
                                    </article>

                                    <article class="timeline-entry begin animate-box" data-animate-effect="fadeInRight">
                                        <div class="timeline-entry-inner">
                                            <div class="timeline-icon color-none">
                                            </div>
                                        </div>
                                    </article>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>
            </div>
        </div>
    </div>

    <script src="js/jquery.min.js"></script>
    <script src="js/jquery.easing.1.3.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/jquery.waypoints.min.js"></script>
    <script src="js/jquery.flexslider-min.js"></script>
    <script src="js/owl.carousel.min.js"></script>
    <script src="js/jquery.countTo.js"></script>
    <script src="js/main.js"></script>
    <!-- <script src="js/index.js"></script> -->
</body>

</html>